AWSTemplateFormatVersion: 2010-09-09
Metadata:
  'AWS::CloudFormation::Interface':
    ParameterGroups:
      - Label:
          default: Required parameters
        Parameters:
          - ObserveStackName
      - Label:
          default: Observe Lambda Configuration
        Parameters:
          - LambdaVersion
          - LambdaReservedConcurrentExecutions
          - LambdaTimeout
          - LambdaMemorySize
          - LambdaS3CustomRules
      - Label:
          default: Retention Options
        Parameters:
          - LogGroupExpirationInDays
Parameters:
  ObserveStackName:
    Type: String
    Description: Stack name for Observe Collection
  LogGroupExpirationInDays:
    Type: Number
    Default: 365
    AllowedValues:
      - 1
      - 3
      - 7
      - 14
      - 30
      - 90
      - 365
    Description: |
      Expiration to set on log groups
  LambdaVersion:
    Type: String
    Default: latest
    Description: Parquet lambda function version
  LambdaReservedConcurrentExecutions:
    Type: Number
    Default: 100
    Description: The number of simultaneous executions to reserve for the function. Set to -1 to not reserve concurrent executions.
  LambdaTimeout:
    Type: Number
    Default: 300
    Description: >-
      The amount of time that Lambda allows a function to run before stopping
      it. The maximum allowed value is 900 seconds.
  LambdaMemorySize:
    Type: Number
    Default: 2048
    MinValue: 128
    MaxValue: 10240
    Description: >-
      The amount of memory that your function has access to. The value must be a
      multiple of 64 MB.
Conditions:
  HasReservedConcurrency: !Not
    - !Equals
      - Ref: LambdaReservedConcurrentExecutions
      - -1
Resources:
  DeliveryStreamPolicy:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: DeliveryStreamPolicy
      Roles:
        - !Ref LambdaRole
      PolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Action:
              - 'firehose:PutRecord'
              - 'firehose:PutRecordBatch'
            Resource: !ImportValue
              "Fn::Sub": "${ObserveStackName}:firehose:arn"
  LambdaLogGroup:
    Type: 'AWS::Logs::LogGroup'
    Properties:
      LogGroupName: !Join
        - ''
        - - /aws/lambda/
          - !Ref 'AWS::StackName'
      RetentionInDays: !Ref LogGroupExpirationInDays
  Lambda:
    Type: 'AWS::Lambda::Function'
    DependsOn:
      - LambdaLogGroup
    Properties:
      FunctionName: !Ref 'AWS::StackName'
      Handler: index.handler
      Role: !GetAtt LambdaRole.Arn
      Environment:
        Variables:
          DELIVERY_STREAM_NAME: !Select
           - "1"
           - !Split
             - "/"
             - !ImportValue
              "Fn::Sub": "${ObserveStackName}:firehose:arn"
      Layers:
        - !Sub "arn:aws:lambda:${AWS::Region}:770693421928:layer:Klayers-p38-pyarrow:5"
      Code:
        ZipFile: |
          import concurrent.futures
          import collections
          import json
          import logging
          from io import BytesIO
          import os
          import gzip
          import random
          import sys
          import threading
          import time

          import boto3

          import pyarrow
          from pyarrow import csv, fs
          from pyarrow.parquet import ParquetFile

          MAX_OBSERVATION_SIZE = 1e6
          MAX_KINESIS_RECORD_SIZE = 1e6


          class Env(object):
              def __init__(self):
                  self.LOG_LEVEL = {
                      "": logging.INFO,
                      "DEBUG": logging.DEBUG,
                      "INFO": logging.INFO,
                      "WARNING": logging.WARNING,
                  }.get(os.environ.get("LOG_LEVEL", ""))

                  if self.LOG_LEVEL is None:
                      raise ValueError(
                          "unknown value provided for LOG_LEVEL. Please select one of DEBUG, INFO or WARNING"
                      )

                  # batch size dictates number of rows from input that end up in a single Kinesis batch PUT (max size: 4MiB)
                  self.BATCH_ROWS = int(os.environ.get("BATCH_ROWS", "100000"))
                  # record size dictates number of rows from input that end up in a single Kinesis record (max size: 1MiB)
                  self.RECORD_ROWS = int(os.environ.get("RECORD_ROWS", "1000"))
                  # 0 is no compression, 9 is max
                  self.COMPRESS_LEVEL = int(os.environ.get("COMPRESS_LEVEL", "3"))

                  self.MAX_WORKERS = int(os.environ.get("MAX_WORKERS", "0")) or None

                  self.MIN_BACKOFF = float(os.environ.get("MIN_BACKOFF", "1"))
                  self.RETRY_COUNT = int(os.environ.get("RETRY_COUNT", "5"))

                  self.DELIVERY_STREAM_NAME = os.environ.get("DELIVERY_STREAM_NAME")
                  self.STREAM_NAME = os.environ.get("STREAM_NAME")

                  if self.DELIVERY_STREAM_NAME and self.STREAM_NAME:
                      raise ValueError(
                          "only one of DELIVERY_STREAM_NAME and STREAM_NAME can be defined"
                      )


          try:
              ENV = Env()
          except ValueError as e:
              sys.exit(e)

          logging.getLogger().setLevel(ENV.LOG_LEVEL)
          if len(logging.getLogger().handlers) > 0:
              # handler already configured in lambda
              logging.getLogger().setLevel(ENV.LOG_LEVEL)
          else:
              logging.basicConfig(level=ENV.LOG_LEVEL, format="%(asctime)s %(message)s")

          # disable dumping request payload
          logging.getLogger("botocore").setLevel(logging.CRITICAL)

          LOCAL = threading.local()


          def init_session():
              session = boto3.session.Session()
              if ENV.DELIVERY_STREAM_NAME:
                  LOCAL.uploader = FirehoseUploader(session)
              elif ENV.STREAM_NAME:
                  LOCAL.uploader = KinesisUploader(session)
              else:
                  # noop
                  LOCAL.uploader = Uploader()


          def convert_to_records(batch, metadata={}):
              records = []

              observation_bytes = 0
              record_bytes = 0

              for offset in range(0, batch.num_rows, ENV.RECORD_ROWS):
                  chunk = batch.slice(offset, length=ENV.RECORD_ROWS)

                  buffer = BytesIO()
                  csv.write_csv(chunk, buffer)
                  buffer.seek(0)
                  observation = json.dumps({"csv": buffer.read().decode("utf-8"), **metadata})

                  record = gzip.compress(
                      observation.encode("utf-8"), compresslevel=ENV.COMPRESS_LEVEL
                  )
                  records.append({"Data": record})

                  observation_len = len(record)
                  if observation_len > MAX_OBSERVATION_SIZE:
                      raise Exception("observation too large", observation_len)

                  record_len = len(record)
                  if record_len > MAX_KINESIS_RECORD_SIZE:
                      raise Exception("record too long", record_len)

                  observation_bytes += observation_len
                  record_bytes += record_len

              count = len(records)

              logging.debug(
                  f"processed batch into records count={count} bytes={record_bytes} observation_bytes={observation_bytes}"
              )

              LOCAL.uploader.upload(records)
              return count


          class Uploader(object):
              def upload(self, records):
                  base_backoff = ENV.MIN_BACKOFF
                  for i in range(0, ENV.RETRY_COUNT + 1):
                      records = self._upload(records)
                      if not records:
                          return []
                      jitter = round(random.random(), 2)
                      backoff = base_backoff * (1 + jitter)
                      logging.info(
                          f"throttled retry={i+1} count={len(records)} backoff={backoff}"
                      )
                      time.sleep(backoff)
                      base_backoff *= 2
                  return records

              def _upload(self, records):
                  return []


          class FirehoseUploader(Uploader):
              def __init__(self, session):
                  self.client = session.client("firehose")

              def _upload(self, records):
                  resp = self.client.put_record_batch(
                      DeliveryStreamName=ENV.DELIVERY_STREAM_NAME, Records=records
                  )
                  if not resp["FailedPutCount"]:
                      return []

                  throttled = [
                      record
                      for record, response in zip(records, resp["RequestResponses"])
                      if response.get("ErrorMessage") == "Slow down."
                  ]
                  if len(throttled) != resp["FailedPutCount"]:
                      raise ValueError(resp)
                  return throttled


          class KinesisUploader(Uploader):
              def __init__(self, session):
                  self.client = session.client("kinesis")

              def _upload(self, records):
                  for record in records:
                      record["PartitionKey"] = str(hash(record["Data"]))

                  resp = self.client.put_records(StreamName=ENV.STREAM_NAME, Records=records)
                  if not resp["FailedRecordCount"]:
                      return []
                  throttled = [
                      record
                      for record, response in zip(records, resp["Records"])
                      if response.get("ErrorCode") == "ProvisionedThroughputExceededException"
                  ]
                  if len(throttled) != resp["FailedRecordCount"]:
                      raise ValueError(resp)
                  return throttled


          def upload(context, filesystem, path):
              count = 0
              with filesystem.open_input_file(path) as f:
                  parquet_file = ParquetFile(f, pre_buffer=True)
                  batches = parquet_file.iter_batches(ENV.BATCH_ROWS)
                  with concurrent.futures.ThreadPoolExecutor(
                      max_workers=ENV.MAX_WORKERS, initializer=init_session
                  ) as executor:
                      for result in executor.map(convert_to_records, batches):
                          count += result
              logging.info(
                  f"done uploading records={count} rows={parquet_file.metadata.num_rows}"
              )


          # handler is invoked from AWS lambda
          def handler(event, context):
              for record in event.get("Records", []):
                  try:
                      bucket_name = record["s3"]["bucket"]["name"]
                      object_key = record["s3"]["object"]["key"]
                  except KeyError:
                      continue
                  s3, path = fs.S3FileSystem().from_uri(f"s3://{bucket_name}/{object_key}")

                  try:
                      upload(context, s3, path)
                  except Exception as e:
                      logging.info(f"failed processing record={json.dumps(record)}")
                      raise e


          if __name__ == "__main__":
              filename = sys.argv[1]
              if filename.startswith("s3"):
                  s3, path = fs.S3FileSystem().from_uri(filename)
                  upload({}, s3, path)
              else:
                  absfilename = os.path.abspath(filename)
                  local, path = fs.LocalFileSystem().from_uri("file:" + absfilename)
                  upload({}, local, path)

      Runtime: python3.8
      MemorySize: !Ref LambdaMemorySize
      Timeout: !Ref LambdaTimeout
      ReservedConcurrentExecutions: !If
       - HasReservedConcurrency
       - !Ref LambdaReservedConcurrentExecutions
       - !Ref 'AWS::NoValue'
  LambdaRole:
    Type: 'AWS::IAM::Role'
    Properties:
      AssumeRolePolicyDocument:
        Version: 2012-10-17
        Statement:
          - Effect: Allow
            Principal:
              Service:
                - lambda.amazonaws.com
            Action:
              - 'sts:AssumeRole'
      Path: /
      ManagedPolicyArns:
        - 'arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole'
  LambdaReadBucket:
    Type: 'AWS::IAM::Policy'
    Properties:
      PolicyName: AllowS3Read
      PolicyDocument:
        Version: "2012-10-17"
        Statement:
          - Effect: Allow
            Action:
              - 's3:Get*'
              - 's3:List*'
            Resource:
              - 'arn:aws:s3:::*'
      Roles:
        - !Ref LambdaRole
  LambdaS3Permission:
    Type: 'AWS::Lambda::Permission'
    Properties:
      Action: 'lambda:InvokeFunction'
      FunctionName: !Ref Lambda
      Principal: s3.amazonaws.com
      SourceAccount: !Ref 'AWS::AccountId'
Outputs:
  LambdaName:
    Description: 'Lambda Name'
    Value: !Ref 'Lambda'
    Export:
      Name: !Sub '${AWS::StackName}:lambda:name'
  LambdaArn:
    Description: 'Lambda ARN'
    Value: !GetAtt 'Lambda.Arn'
    Export:
      Name: !Sub '${AWS::StackName}:lambda:arn'
